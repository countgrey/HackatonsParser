import requests
from bs4 import BeautifulSoup
import sqlite3
import re
from urllib.parse import urljoin, urlparse
import time
import math
import json
from datetime import datetime
from collections import deque

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
DATABASE_NAME = "events.db"
SOURCES_FILE = "sources.json" 

# --- –ü–ê–†–ê–ú–ï–¢–†–´ –ó–ê–ü–£–°–ö–ê ---
# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø—Ä–æ—Ö–æ–¥ —Ç–æ–ª—å–∫–æ –ø–æ –ø–µ—Ä–≤–æ–π —Ç—Ä–µ—Ç–∏ (1/3) –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö —Å—Å—ã–ª–æ–∫.
TEST_LIMIT_FRACTION = 1
# –ï—Å–ª–∏ True, –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º.
DEBUG = True
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ö–æ–¥–∞
MAX_CRAWL_PAGES = 10

# --- –§–ò–õ–¨–¢–†–´ –ò –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ---
# –°–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫ (–≠—Ç–∞–ø 1)
EVENT_KEYWORDS = [
    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', '—Å–µ–º–∏–Ω–∞—Ä', '—Ö–∞–∫–∞—Ç–æ–Ω', '–∫–æ–Ω–∫—É—Ä—Å', '—Ñ–æ—Ä—É–º', 
    '—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ', '–æ–ª–∏–º–ø–∏–∞–¥–∞', '–≤—ã—Å—Ç–∞–≤–∫–∞', '–ª–µ–∫—Ü–∏—è', '–≤–µ–±–∏–Ω–∞—Ä',
    '–ø—Ä–æ–π–¥–µ—Ç' # –ß–∞—Å—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∞–Ω–æ–Ω—Å
]

# –¢–∏–ø—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
EVENT_TYPES_MAP = {
    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü': '–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è',
    '—Å–µ–º–∏–Ω–∞—Ä': '–°–µ–º–∏–Ω–∞—Ä',
    '—Ö–∞–∫–∞—Ç–æ–Ω': '–•–∞–∫–∞—Ç–æ–Ω',
    '–∫–æ–Ω–∫—É—Ä—Å': '–ö–æ–Ω–∫—É—Ä—Å/–°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ',
    '–æ–ª–∏–º–ø–∏–∞–¥': '–û–ª–∏–º–ø–∏–∞–¥–∞',
    '–≤—ã—Å—Ç–∞–≤–∫': '–í—ã—Å—Ç–∞–≤–∫–∞',
    '—Ñ–æ—Ä—É–º': '–§–æ—Ä—É–º'
}

## <-- –ù–û–í–û–ï: –≠–í–†–ò–°–¢–ò–ß–ï–°–ö–ò–ô –§–ò–õ–¨–¢–†
# –°—Ç–æ–ø-—Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –ø—Ä–æ—à–µ–¥—à–µ–µ —Å–æ–±—ã—Ç–∏–µ –∏–ª–∏ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π "—à—É–º"
NOISE_INDICATORS = [
    '—Å–æ—Å—Ç–æ—è–ª–∞—Å—å', '–ø—Ä–æ—à–µ–ª', '–ø–æ–¥–≤–µ–ª–∏ –∏—Ç–æ–≥–∏', '–∏—Ç–æ–≥–∏ –∫–æ–Ω–∫—É—Ä—Å–∞', '–∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å',
    '–ø–æ–±–µ–¥–∏—Ç–µ–ª—å', '–ª–∞—É—Ä–µ–∞—Ç', '–ø—Ä–∏–∑–µ—Ä–æ–≤', '–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ', '–æ—Ç–∫—Ä—ã—Ç –ø—Ä–∏–µ–º', # "–æ—Ç–∫—Ä—ã—Ç –ø—Ä–∏–µ–º" –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º, –Ω–æ —á–∞—Å—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –Ω–æ–≤–æ—Å—Ç—è–º
    '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–∫–∞—Ñ–µ–¥—Ä–∞', '–ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º', '–≤–æ—à–µ–ª –≤ —Ç–æ–ø', '–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å'
]
# –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–∞–º–∏ –ø–æ —Å–µ–±–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–æ–±—ã—Ç–∏–µ–º (—Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
NON_EVENT_TERMS = [
    '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '—Ñ–∞–∫—É–ª—å—Ç–µ—Ç', '–æ –Ω–∞—Å', '–Ω–æ–≤–æ—Å—Ç–∏ –≤—É–∑–∞', '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞'
]
# –ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
KEYWORD_DENSITY_THRESHOLD = 0.005 # 0.5% –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
## ---------------------------------

# --- –§—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ---

def load_sources():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑ sources.json."""
    try:
        with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
            sources = json.load(f)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ {SOURCES_FILE}.")
        return sources
    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª '{SOURCES_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–π—Ç–µ –µ–≥–æ.")
        return []
    except json.JSONDecodeError:
        print(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –≤ —Ñ–∞–π–ª–µ '{SOURCES_FILE}'.")
        return []

# --- –§—É–Ω–∫—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---

def clear_database():
    """–£–¥–∞–ª—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É events, –æ—á–∏—â–∞—è –≤—Å–µ –¥–∞–Ω–Ω—ã–µ."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    cursor.execute("DROP TABLE IF EXISTS events")
    conn.commit()
    conn.close()
    print("üóëÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–µ–Ω–∞ (DEBUG=True).")

def setup_database():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å–æ–±—ã—Ç–∏–π –≤ SQLite, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS events (
            id INTEGER PRIMARY KEY,
            title TEXT,
            city TEXT,
            type TEXT,
            date_start TEXT,
            date_end TEXT,
            reg_start TEXT,
            reg_end TEXT,
            team_required BOOLEAN,
            audience TEXT,
            organizer TEXT,
            link TEXT UNIQUE,
            text TEXT 
        )
    """)
    conn.commit()
    conn.close()
    print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{DATABASE_NAME}' –≥–æ—Ç–æ–≤–∞.")

def save_event(event_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ audience –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ JSON-—Å—Ç—Ä–æ–∫—É
        audience_json = json.dumps(event_data.get('audience', []))
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ team_required –≤ —á–∏—Å–ª–æ (0 –∏–ª–∏ 1)
        team_required_val = 1 if event_data.get('team_required') else 0

        cursor.execute("""
            INSERT INTO events (
                title, city, type, date_start, date_end, reg_start, reg_end,
                team_required, audience, organizer, link, text
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            event_data.get('title'),
            event_data.get('city'),
            event_data.get('type'),
            event_data.get('date_start'),
            event_data.get('date_end'),
            event_data.get('reg_start'),
            event_data.get('reg_end'),
            team_required_val, 
            audience_json, 
            event_data.get('organizer'),
            event_data.get('link'),
            event_data.get('text') 
        ))
        conn.commit()
        print(f"    üíæ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {event_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}...")
        return True
    except sqlite3.IntegrityError:
        print("    ‚û°Ô∏è –ü—Ä–æ–ø—É—Å–∫: –°—Å—ã–ª–∫–∞ —É–∂–µ –µ—Å—Ç—å –≤ –ë–î.")
        return False
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        return False
    finally:
        conn.close()

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –æ–±—Ö–æ–¥–∞ (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---

def fetch_and_extract_text(url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç."""
    # (–§—É–Ω–∫—Ü–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç page_title –∏ full_text_to_search)
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        html_content = response.text
    except requests.exceptions.RequestException:
        return None, None

    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. –£–¥–∞–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤-—à—É–º–æ–≤ –ø–æ —Ç–µ–≥–∞–º
    for element in soup(["header", "footer", "nav", "aside", "script", "style", "img", "form", "button", "iframe"]):
        element.decompose()
        
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤-—à—É–º–æ–≤ –ø–æ –æ–±—â–∏–º –∫–ª–∞—Å—Å–∞–º
    noise_selectors = [
        '.sidebar', '.nav', '.menu', '#nav', '#menu', '.advertisement', 
        '.footer', '.widget', '.vacancies', '#footer', '#header', '#sidebar', 
        '.cookie-notice', '#cookie-banner', '.gdpr-container', '#privacy-policy'
    ]
    for selector in noise_selectors:
        for element in soup.select(selector):
            element.decompose()
        
    # 3. –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (Title)
    page_title_tag = soup.find('h1') or soup.find('title')
    page_title = page_title_tag.get_text(strip=True) if page_title_tag else ""

    # 4. –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    main_content = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile(r'(content|main-content|article-content|post|entry|text-block)', re.I))
    
    if main_content:
        text = main_content.get_text(separator=' ', strip=True)
    else:
        text = soup.get_text(separator=' ', strip=True)
        
    cleaned_text = re.sub(r'\s+', ' ', text)
    
    # 5. –£–±–∏—Ä–∞–µ–º –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1, –µ—Å–ª–∏ –æ–Ω –±—ã–ª –Ω–∞–π–¥–µ–Ω
    if page_title and cleaned_text.startswith(page_title):
        cleaned_text = cleaned_text[len(page_title):].strip()
        
    full_text_to_search = page_title + " " + cleaned_text
    
    # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ (–∫—Ä–æ–º–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞), —ç—Ç–æ, –≤–µ—Ä–æ—è—Ç–Ω–æ, –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
    if len(cleaned_text.split()) < 5:
        return None, None
        
    return page_title, full_text_to_search[:8000]

# (–§—É–Ω–∫—Ü–∏–∏ –æ–±—Ö–æ–¥–∞ –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def check_link_relevance_by_keywords(context):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è."""
    context_lower = context.lower()
    
    # –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    is_event = any(keyword in context_lower for keyword in EVENT_KEYWORDS)
    
    # –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —à—É–º
    is_noise = any(noise in context_lower for noise in ['—Ä–µ–∫–≤–∏–∑–∏—Ç—ã', '–≤–∞–∫–∞–Ω—Å–∏–∏', '—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å', '–ø—Ä–∏–∫–∞–∑', '–ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º'])
    
    return is_event and not is_noise

def extract_links_from_page(url, base_url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    base_domain = urlparse(base_url).netloc
    
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException:
        return set(), set()

    soup = BeautifulSoup(response.text, 'html.parser')
    all_links = soup.find_all('a', href=True)
    
    event_links = set()
    crawl_links = set()
    file_extensions = ['.pdf', '.doc', '.docx', '.zip', '.rar', '#', '.xlsx', '.jpg', '.jpeg', '.png']
    
    for link in all_links:
        href = link['href']
        full_url = urljoin(base_url, href).split('#')[0] 
        
        # 1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤, —è–∫–æ—Ä–µ–π –∏ –≤–Ω–µ—à–Ω–∏—Ö/—Ç–µ–∫—É—â–∏—Ö —Å—Å—ã–ª–æ–∫
        if any(full_url.lower().endswith(ext) for ext in file_extensions) or \
           urlparse(full_url).netloc != base_domain or \
           full_url == url:
            continue
            
        link_text = link.get_text(strip=True)
        context = link_text
        
        parent = link.find_parent()
        if parent and parent.name in ['li', 'p', 'div', 'td']:
            context = parent.get_text(strip=True)
            
        # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if check_link_relevance_by_keywords(context):
            event_links.add(full_url)
        
        # –í—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–º –∂–µ –¥–æ–º–µ–Ω–µ —Å—á–∏—Ç–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞
        crawl_links.add(full_url)
        
    return event_links, crawl_links

def crawl_site_bfs(start_url, base_url, max_pages):
    """–û–±—Ö–æ–¥ —Å–∞–π—Ç–∞ –≤ —à–∏—Ä–∏–Ω—É (BFS)."""
    
    # –û—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ö–æ–¥–∞ (FIFO)
    queue = deque([start_url])
    # –í—Å–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    visited_pages = {start_url}
    # –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ–±—ã—Ç–∏—è
    all_event_links = set()
    
    pages_crawled = 0
    
    print(f"\n--- –≠–¢–ê–ü 1: –û–±—Ö–æ–¥ —Å–∞–π—Ç–∞ (BFS) –¥–ª—è {base_url} ---")
    
    while queue and pages_crawled < max_pages:
        current_url = queue.popleft()
        pages_crawled += 1
        print(f"    [–û–±—Ö–æ–¥ {pages_crawled}/{max_pages}] -> {current_url}")
        
        event_links, crawl_links = extract_links_from_page(current_url, base_url)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ–±—ã—Ç–∏—è –≤ –æ–±—â–∏–π –Ω–∞–±–æ—Ä
        all_event_links.update(event_links)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤ –æ—á–µ—Ä–µ–¥—å
        for link in crawl_links:
            if link not in visited_pages:
                visited_pages.add(link)
                queue.append(link)
                
    print(f"‚úÖ –û–±—Ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ü–æ—Å–µ—â–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages_crawled}. –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ–±—ã—Ç–∏—è: {len(all_event_links)}")
    return list(all_event_links)

# --- –≠–¢–ê–ü 2 –∏ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º —Ñ–∏–ª—å—Ç—Ä–æ–º ---

def parse_dates(text):
    # (–§—É–Ω–∫—Ü–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    dates = []
    
    # 1. –ü–æ–∏—Å–∫ –≤ —è–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–¥–¥.–º–º.–≥–≥–≥–≥)
    explicit_dates = re.findall(r'(\d{1,2}\.\d{1,2}\.\d{2,4})', text)
    for date_str in explicit_dates:
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å, —á—Ç–æ–±—ã –æ—Ç—Å–µ—è—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞—Ç—ã
            dt = datetime.strptime(date_str, '%d.%m.%Y')
            dates.append(dt.strftime('%Y-%m-%d'))
        except ValueError:
            pass

    # 2. –ü–æ–∏—Å–∫ –≤ —Å–ª–æ–≤–µ—Å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    month_map = {'—è–Ω–≤': 1, '—Ñ–µ–≤': 2, '–º–∞—Ä': 3, '–∞–ø—Ä': 4, '–º–∞–π': 5, '–∏—é–Ω': 6, 
                 '–∏—é–ª': 7, '–∞–≤–≥': 8, '—Å–µ–Ω': 9, '–æ–∫—Ç': 10, '–Ω–æ—è': 11, '–¥–µ–∫': 12}
    date_patterns = r'(\d{1,2})[\.\s](—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*[\.\s]?(\d{2,4})'
                 
    verbal_dates = re.findall(date_patterns, text, re.I)
    
    for day, month_abbr, year in verbal_dates:
        try:
            month_num = month_map[month_abbr[:3].lower()]
            if len(year) == 2:
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ 20XX –≥–æ–¥
                year = '20' + year
                
            dt = datetime(int(year), month_num, int(day))
            dates.append(dt.strftime('%Y-%m-%d'))
        except:
            pass

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    unique_dates = sorted(list(set(dates)))
    
    date_start = unique_dates[0] if unique_dates else ""
    date_end = unique_dates[-1] if len(unique_dates) > 1 else date_start
    
    return date_start, date_end

def check_event_relevance_by_heuristics(page_title, page_text):
    """
    –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç False, –µ—Å–ª–∏ —Å–æ–±—ã—Ç–∏–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ.
    """
    context_lower = (page_title + " " + page_text).lower()
    total_length = len(context_lower.split())

    # 1. –§–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º (–Ω–æ–≤–æ—Å—Ç–∏/–ø—Ä–æ—à–µ–¥—à–∏–µ —Å–æ–±—ã—Ç–∏—è)
    if any(noise_term in context_lower for noise_term in NOISE_INDICATORS):
        return False, "–ù–∞–π–¥–µ–Ω –Ω–æ–≤–æ—Å—Ç–Ω–æ–π/–ø—Ä–æ—à–µ–¥—à–∏–π –º–∞—Ä–∫–µ—Ä"

    # 2. –§–∏–ª—å—Ç—Ä –ø–æ –Ω–µ-—Å–æ–±—ã—Ç–∏–π–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–∞–º (—Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–∞–º –ø–æ —Å–µ–±–µ –Ω–µ-—Å–æ–±—ã—Ç–∏–π–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–æ–º
    if any(term in page_title.lower() for term in NON_EVENT_TERMS):
        return False, "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–ª—É–∂–µ–±–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"
        
    # 3. –§–∏–ª—å—Ç—Ä –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    event_keyword_count = sum(context_lower.count(keyword) for keyword in EVENT_KEYWORDS)
    if total_length > 100 and event_keyword_count / total_length < KEYWORD_DENSITY_THRESHOLD:
        return False, "–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"

    # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä: –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç –∏ –Ω–µ—Ç —Ç–∏–ø–∞ —Å–æ–±—ã—Ç–∏—è, —Å—á–∏—Ç–∞—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º
    # (–≠—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –≤ –≤—ã–∑—ã–≤–∞—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏, –ø–æ—Å–ª–µ extract_event_data_python_only)

    return True, "–ü—Ä–æ–π–¥–µ–Ω"


def extract_event_data_python_only(page_title, page_text, original_link, default_organizer, default_city):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é —á–∏—Å—Ç–æ–≥–æ Python (RegEx).
    """
    # <-- –ù–û–í–û–ï: –ü–†–û–í–ï–†–ö–ê –≠–í–†–ò–°–¢–ò–ß–ï–°–ö–ò–ú –§–ò–õ–¨–¢–†–û–ú
    is_relevant, reason = check_event_relevance_by_heuristics(page_title, page_text)
    if not is_relevant:
        print(f"    ‚ùå –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä: –ü—Ä–æ–ø—É—Å–∫. –ü—Ä–∏—á–∏–Ω–∞: {reason}")
        return None
    # ---------------------------------------------
    
    event_data = {
        "title": page_title.strip() if page_title else "",
        "city": default_city, # <-- –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        "type": "–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ",
        "date_start": "",
        "date_end": "",
        "reg_start": "",
        "reg_end": "",
        "team_required": False,
        "audience": [],
        "organizer": default_organizer, # <-- –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        "link": original_link,
        "text": page_text # <-- –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    }
    
    # 1. –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: –ï—Å–ª–∏ H1 –ø—É—Å—Ç, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –æ—Å–º—ã—Å–ª–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É
    if not event_data['title']:
        first_sentence = re.split(r'[.!?]', page_text, 1)[0]
        if len(first_sentence.split()) > 3:
              event_data['title'] = first_sentence.strip()
    
    # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
    for keyword, event_type in EVENT_TYPES_MAP.items():
        if re.search(keyword, page_text, re.I):
            event_data['type'] = event_type
            break
            
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç
    start, end = parse_dates(page_text)
    event_data['date_start'] = start
    event_data['date_end'] = end
    
    # 4. –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞, –∞—É–¥–∏—Ç–æ—Ä–∏–∏, –∫–æ–º–∞–Ω–¥—ã (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    if re.search(r'–æ—Ä–µ–Ω–±—É—Ä–≥', page_text, re.I):
        event_data['city'] = "–û—Ä–µ–Ω–±—É—Ä–≥"
        
    if re.search(r'–æ—Ä–µ–Ω–±—É—Ä–≥—Å–∫–∏–π –≥–æ—Å—É–¥|–æ–≥—É|–æ—Ä–µ–Ω–±—É—Ä–≥—Å–∫–∏–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', page_text, re.I):
        event_data['organizer'] = "–û–ì–£"
    
    audience_map = {'—Å—Ç—É–¥–µ–Ω—Ç': '—Å—Ç—É–¥–µ–Ω—Ç', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å': '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å', '—à–∫–æ–ª—å–Ω–∏–∫': '—à–∫–æ–ª—å–Ω–∏–∫', '–Ω–∞—É—á–Ω—ã–π': '–Ω–∞—É—á–Ω—ã–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–∞—Å–ø–∏—Ä–∞–Ω—Ç': '—Å—Ç—É–¥–µ–Ω—Ç'}
    found_audience = set()
    for keyword, audience_type in audience_map.items():
        if re.search(keyword, page_text, re.I):
            found_audience.add(audience_type)
    event_data['audience'] = list(found_audience)
    
    reg_match = re.search(r'(—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è|–∑–∞—è–≤–∫–∏)\s+(?:–¥–æ|–ø–æ)\s+(\d{1,2}\.\d{1,2}\.\d{2,4})', page_text, re.I)
    if reg_match:
        try:
            event_data['reg_end'] = datetime.strptime(reg_match.group(2), '%d.%m.%Y').strftime('%Y-%m-%d')
        except ValueError:
            pass
            
    if re.search(r'–∫–æ–º–∞–Ω–¥[–∞—ã—É]', page_text, re.I):
        event_data['team_required'] = True
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –Ω–∏ –¥–∞—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–æ—Å—Ç–∞–µ—Ç—Å—è)
    if not event_data['title'] and not event_data['date_start']:
        return None
        
    return event_data

# --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–∞—Ä—Å–µ—Ä–∞ (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---

def run_parser():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –æ–±—Ö–æ–¥–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤."""
    
    # 1. –û—á–∏—Å—Ç–∫–∞ –ë–î (–µ—Å–ª–∏ DEBUG=True) –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    if DEBUG:
        clear_database()
        
    setup_database()
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    sources = load_sources()
    if not sources:
        print("üî¥ –ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
        
    global_events_saved = 0
    
    for source in sources:
        source_name = source['name']
        start_url = source['start_url']
        base_url = source['base_url']
        default_city = source['city']
        default_organizer = source_name
        
        print(f"\n=======================================================")
        print(f"                –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –ò–°–¢–û–ß–ù–ò–ö–ê: {source_name}")
        print(f"=======================================================")
        
        # 3. –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ —Å –ø–æ–º–æ—â—å—é –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞ (BFS)
        target_links = crawl_site_bfs(start_url, base_url, MAX_CRAWL_PAGES)
        
        if not target_links:
            print(f"üî¥ –î–ª—è {source_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫.")
            continue
        
        # --- –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï –î–õ–Ø –¢–ï–°–¢–ê ---
        limit = math.ceil(len(target_links) * TEST_LIMIT_FRACTION)
        links_to_process = target_links[:limit]
        total_target_links = len(target_links)
        
        print(f"\n--- –≠–¢–ê–ü–´ 2 –∏ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {source_name} ---")
        print(f"üéØ –í—ã–±—Ä–∞–Ω–æ {len(links_to_process)}/{total_target_links} —Ü–µ–ª–µ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ª–∏–º–∏—Ç: {TEST_LIMIT_FRACTION * 100:.0f}%).")
        
        links_processed = 0
        events_saved = 0
        
        for i, link in enumerate(links_to_process):
            links_processed += 1
            print(f"\n* –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Å—ã–ª–∫–∏ {i + 1}/{len(links_to_process)}: {link}")
            
            # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_title, page_content = fetch_and_extract_text(link)
            
            if not page_content:
                print("    ‚è© –ü—Ä–æ–ø—É—Å–∫ (–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å/–æ—á–∏—Å—Ç–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω).")
                continue
                
            print("    üîç Python/RegEx: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            
            # 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (Python/RegEx) —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            event_data = extract_event_data_python_only(
                page_title, 
                page_content, 
                link, 
                default_organizer, 
                default_city
            )
            
            if event_data:
                # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
                if save_event(event_data):
                    events_saved += 1
            else:
                # –≠—Ç–æ—Ç –±–ª–æ–∫ —Ç–µ–ø–µ—Ä—å –ª–æ–≤–∏—Ç –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, —Ç–∞–∫ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
                pass 
                
            time.sleep(0.1) 
            
        global_events_saved += events_saved
        
        print(f"\n--- –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø {source_name} ---")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {links_processed}/{total_target_links}.")
        print(f"üíæ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {events_saved}.")

    print(f"\n=======================================================")
    print(f"                             –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print(f"=======================================================")
    print(f"üíæ –í–°–ï–ì–û –£–°–ü–ï–®–ù–û –°–û–•–†–ê–ù–ï–ù–û –°–û–ë–´–¢–ò–ô: {global_events_saved}.")


if __name__ == "__main__":
    run_parser()